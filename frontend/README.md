# YouTube Transcript Analyzer - Frontend

The React frontend for the YouTube Transcript Analyzer application, built with React, TypeScript, and Vite.

<div align="center">
  <img src="./public/vite.svg" alt="YouTube Transcript Analyzer Logo" width="100" />
</div>

## Features

- Modern, responsive user interface
- Real-time processing status updates
- Interactive video list with filtering
- Detailed video analysis view with Markdown rendering
- Smart handling of structured AI data
- Client-side routing with React Router
- API integration with the FastAPI backend

## Technologies Used

- **React** - UI library
- **TypeScript** - Type-safe JavaScript
- **Vite** - Fast build tool and dev server
- **React Router** - Client-side routing
- **React Markdown** - Markdown rendering
- **Axios** - HTTP client for API calls
- **CSS Modules** - Scoped styling

## Project Structure

```
frontend/
├── public/               # Static assets
├── src/
│   ├── components/       # React components
│   │   ├── Home.tsx      # Home page component
│   │   ├── VideoDetail.tsx # Video details page
│   │   ├── VideoList.tsx # Video list component
│   │   └── ...           # Other components
│   ├── types/            # TypeScript type definitions
│   ├── App.tsx           # Main application component
│   ├── main.tsx          # Application entry point
│   └── index.css         # Global styles
├── .env                  # Environment variables
├── package.json          # Dependencies and scripts
├── tsconfig.json         # TypeScript configuration
├── vite.config.ts        # Vite configuration
└── README.md             # This file
```

## Project Logo

The project uses a custom SVG logo located at `public/vite.svg`. The logo features:
- A red background representing YouTube's brand color
- A play button icon symbolizing video content
- "Transcript" text to indicate the application's focus on transcript analysis

If you wish to modify the logo:
1. Edit the SVG file directly in `public/vite.svg`
2. The logo is used in the main application header and the README files
3. After updating, rebuild the application to see changes

## Setup and Installation

### Prerequisites

- Node.js 22+
- npm or yarn
- Backend server running (see backend README)

### Installation

1. Clone the repository and navigate to the frontend directory:
   ```bash
   cd frontend
   ```

2. Install dependencies:
   ```bash
   npm install
   # or
   yarn
   ```
3. Start the development server:
   ```bash
   npm run dev
   # or
   yarn dev
   ```

5. Access the application at [http://localhost:5173](http://localhost:5173)

## Development

### Available Scripts

- `npm run dev` - Start the development server
- `npm run build` - Build for production
- `npm run lint` - Run ESLint for code quality
- `npm run preview` - Preview the production build locally

### API Integration

The frontend communicates with the backend API using Axios. Key API endpoints include:

- `POST /process/` - Start processing a YouTube playlist
- `GET /status/{task_id}` - Check processing status
- `GET /videos/` - Get list of processed videos
- `GET /videos/{video_id}` - Get detailed information for a video

### Data Processing

The frontend handles structured data from the backend in several ways:

1. **AI Analysis Rendering** - The VideoDetail component parses and renders structured AI analysis using React Markdown
2. **Category Handling** - Implements robust parsing for categories, handling multiple data formats
3. **Real-time Updates** - Polls for updates during processing and refreshes content automatically

### Authentication Dependencies

The frontend doesn't handle YouTube API authentication directly. Instead, it relies on the backend having a valid `token.pickle` file which is generated by running the `authenticate_youtube.py` script in the backend directory. If you encounter issues with playlist processing or video fetching, ensure that:

1. You've completed the YouTube API authentication step in the backend setup
2. The `token.pickle` file exists in the backend directory
3. Your Google Cloud project has YouTube Data API enabled
4. Your authentication hasn't expired (if it has, run the authentication script again)

The backend will handle all YouTube API requests using this token, so the frontend only needs to communicate with the backend API, not directly with YouTube.

### Component Overview

#### Home Component
The main landing page where users can:
- Enter a YouTube playlist URL or ID
- Start the processing job
- View processing status

#### VideoList Component
Displays all processed videos with:
- Thumbnail image
- Title
- AI verdict
- Category tags
- Filtering options

#### VideoDetail Component
Shows detailed information about a single video:
- Full video metadata
- AI-generated summary with Markdown rendering
- Core topic identification
- Key takeaways
- Content structure
- Category tags
- "Worth watching" verdict with justification
- Processing status (if still in progress)

## Styling

The application uses CSS Modules for component-specific styling. Global styles are defined in `src/index.css`.

To modify the visual appearance:
1. Locate the CSS file for the component you want to change
2. Make your modifications
3. Save and see changes reflected immediately in development

## Building for Production

To create a production build:

```bash
npm run build
# or
yarn build
```

The built files will be in the `dist` directory, ready to be served by any static file server.

## Troubleshooting

### Common Issues

- **API Connection Errors**: Ensure the backend server is running and the `VITE_API_URL` environment variable is correctly set
- **CORS Issues**: Make sure the backend has CORS configured to allow requests from the frontend origin
- **Processing Failures**: Check the backend logs for details on any processing errors
- **Blank Video List**: This could indicate authentication issues on the backend - verify the `token.pickle` file exists and is valid
- **Missing Categories**: If categories are not displaying, check the browser console for errors and ensure the getCategories function in VideoDetail.tsx is correctly parsing the data
